- Meeting with Math√©
	- Don't try to indefinitely optimize the leaking code, you can make the argument for "it works at scale" even if the numbers aren't optimal
	- Start looking at applications/application categories that are interesting targets
		- Operating Systems
		- Webservers
		- Databases
		- K/V stores
		- others?
	- Figure out allocation patterns of these applications
		- For example, Linux always has a fixed value somewhere around the context switch stack with registers that can be used. Scan all pages at that specific offset
		- Run an application, pause the VM and look at the memory layout a couple of times. Did it change? Is there a pattern?
		- Applications are generally closely allocated to each other
	- Core scheduling might be defeated if L1d flushing doesn't happen all the time
		- If you are scheduled right after a victim, data can still be in L1d
		- Still possible to leak if your cache footprint is minimal
	- For speeding up the physical memory scanning, you don't even have to wait for the entire register state to be pushed. You can directly compare and jump since all values are already in the registers. Some alignment for the iret instructions might be necessary though.
- Updated the stable kernel to 6.7.6
- Modified the IDT to check the magic values and instantly change the return address instead of going through all the Linux stuff the pushes and pops stuff
- Tried debugging with GDB but that seems to be a disaster to setup. Still trying though
- Tomorrow, build alpine image in the vm